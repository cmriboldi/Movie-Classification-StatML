{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn.cluster\n",
    "import seaborn\n",
    "import time\n",
    "\n",
    "def timer():\n",
    "    elapsed_time = (time.time() - start)\n",
    "    return \" {0:.2f}\".format(elapsed_time) + \" seconds\" if elapsed_time <= 120 else \" {0:.2f}\".format(elapsed_time/60) + \" minutes\"\n",
    "\n",
    "def create_movie_genre_dict():\n",
    "    movie_keys = mov_genres['movieID'].unique()\n",
    "    movie_genres = dict.fromkeys(movie_keys)\n",
    "    for mov_id in tqdm(movie_keys):\n",
    "        movie_genres[mov_id] = get_genres(mov_id)\n",
    "    return movie_genres\n",
    "\n",
    "def get_genres(movieID):\n",
    "    return pd.Series( [1 if (genre in mov_genres[mov_genres['movieID'] == movieID]['genre'].unique()) else 0 for genre in genres], index=genres)\n",
    "\n",
    "my_test = pd.read_csv('output/test_movies_with_genres.dat', '\\t')\n",
    "my_train = pd.read_csv('output/train_movies_with_genres.dat', '\\t')\n",
    "mov_genres = pd.read_csv('movie_training_data/movie_genres.dat','\\t')\n",
    "predictions = pd.read_csv('predictions.dat', '\\t')\n",
    "genres = mov_genres['genre'].unique()\n",
    "\n",
    "movie_genres = create_movie_genre_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting clusters\n",
      "Created clusters   45.76 seconds\n",
      "Have results\n",
      "             0  1\n",
      "0            0  1\n",
      "1            1  1\n",
      "2            2  6\n",
      "3            3  3\n",
      "4            4  2\n",
      "5            5  8\n",
      "6            6  2\n",
      "7            7  0\n",
      "8            8  3\n",
      "9            9  7\n",
      "10          10  5\n",
      "11          11  2\n",
      "12          12  5\n",
      "13          13  7\n",
      "14          14  7\n",
      "15          15  8\n",
      "16          16  3\n",
      "17          17  5\n",
      "18          18  6\n",
      "19          19  1\n",
      "20          20  6\n",
      "21          21  5\n",
      "22          22  8\n",
      "23          23  5\n",
      "24          24  9\n",
      "25          25  5\n",
      "26          26  2\n",
      "27          27  5\n",
      "28          28  7\n",
      "29          29  3\n",
      "...        ... ..\n",
      "685568  685568  3\n",
      "685569  685569  9\n",
      "685570  685570  0\n",
      "685571  685571  0\n",
      "685572  685572  4\n",
      "685573  685573  7\n",
      "685574  685574  7\n",
      "685575  685575  2\n",
      "685576  685576  5\n",
      "685577  685577  7\n",
      "685578  685578  3\n",
      "685579  685579  7\n",
      "685580  685580  0\n",
      "685581  685581  2\n",
      "685582  685582  6\n",
      "685583  685583  0\n",
      "685584  685584  7\n",
      "685585  685585  7\n",
      "685586  685586  5\n",
      "685587  685587  4\n",
      "685588  685588  0\n",
      "685589  685589  3\n",
      "685590  685590  0\n",
      "685591  685591  8\n",
      "685592  685592  0\n",
      "685593  685593  2\n",
      "685594  685594  5\n",
      "685595  685595  9\n",
      "685596  685596  2\n",
      "685597  685597  4\n",
      "\n",
      "[685598 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "train_data = my_train.drop(my_train.columns[:4], axis=1)\n",
    "\n",
    "start = time.time()\n",
    "M = 10\n",
    "mat_train = train_data.as_matrix()\n",
    "print(\"Starting clusters\")\n",
    "km = sklearn.cluster.KMeans(n_clusters=M)\n",
    "km.fit(mat_train)\n",
    "print(\"Created clusters \", timer())\n",
    "labels = km.labels_\n",
    "results = pd.DataFrame([train_data.index,labels]).T\n",
    "print(\"Have results\")\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def classify(row):\n",
    "    row_genres = row[genres].T\n",
    "    row_genres.columns = range(len(row_genres.columns))\n",
    "    row_genres = row_genres.loc[row_genres[0] == 1].T\n",
    "    possible_clusters = []\n",
    "    for col in row_genres.columns:\n",
    "        possible_clusters.append(np.random.choice(range(M), p=cond_classifier[col]))\n",
    "    return np.random.choice(possible_clusters)\n",
    "\n",
    "def round_of_rating(number):\n",
    "    return round(number * 2) / 2\n",
    "\n",
    "def rate(row):\n",
    "    clust = classify(row)\n",
    "    return round_of_rating(np.random.normal(means[clust], stds[clust]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "results.columns = ['index', 'cluster']\n",
    "\n",
    "clusters = dict.fromkeys(range(0,M))\n",
    "means = dict.fromkeys(range(0,M))\n",
    "stds = dict.fromkeys(range(0,M)) \n",
    "for cluster_idx in tqdm(range(0,M)):\n",
    "    clusters[cluster_idx] = my_train[results['cluster'] == cluster_idx]\n",
    "    means[cluster_idx] = clusters[cluster_idx]['rating'].mean()\n",
    "    stds[cluster_idx] = clusters[cluster_idx]['rating'].std()\n",
    "\n",
    "# Create Conditional Classifier\n",
    "cond_classifier = dict.fromkeys(genres)\n",
    "for genre in genres:\n",
    "    cond_classifier[genre] = []\n",
    "    for cluster_idx, cluster in clusters.items():\n",
    "        cond_classifier[genre].append(cluster[genre].mean())\n",
    "    cond_classifier[genre] /= np.sum(cond_classifier[genre])\n",
    "\n",
    "test_ratings = []\n",
    "for index, row in tqdm(my_test.iterrows()):\n",
    "    test_ratings.append(rate(pd.DataFrame(row).T))\n",
    "\n",
    "my_test[\"pred_ratings\"] = pd.Series( test_ratings, index=my_test.index )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "63.44 %\n",
      "RMSE is:  1.44741811914\n"
     ]
    }
   ],
   "source": [
    "success_rate = 0\n",
    "rmse = []\n",
    "for index, row in tqdm(my_test.iterrows()):\n",
    "    success_rate += 1 if (np.abs(row['rating'] - row['pred_ratings']) <=1) else 0\n",
    "    rmse.append(row['rating'] - row['pred_ratings'])\n",
    "print((success_rate / len(my_test.index)) * 100, '%')\n",
    "print(\"RMSE is: \", np.std(rmse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "rating_predictions = []\n",
    "for index, row in tqdm(predictions.iterrows()):\n",
    "    rating_predictions.append(rate(pd.DataFrame(movie_genres[row['movieID']]).T))\n",
    "\n",
    "predictions['predicted rating'] = pd.Series( rating_predictions, index=predictions.index )\n",
    "\n",
    "predictions.to_csv('output/predictions.dat', sep='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Project Write-up\n",
    "\n",
    "1. Exploratory data analysis\n",
    "   In order to better understand the data that I was dealing with I first began to graph a number of histograms along different axes. I saw that there was a strong correlation between the number of ratings and the months of December and January. But honestly it wasn't a ton to go on so I began to search through a few different ideas for how I could create my recommender system. I found a few good tutorials but most of them didn't have exactly what I needed so eventually I decided to create my own hybrid system. It's an Item-Item Conditional Classifier System based exclusively on the genre.\n",
    "\n",
    "2. Description of technical approach \n",
    "   The reason that I went with a Item-Item recommender system is because it's very fast after the preliminary training and you don't have to know anything about the users. While exploring the data I realized that I know almost nothing about the users who created these ratings while I know a whole lot more about the movies themselves. So what my system is doing is trying to find patterns in the data that I can then use to create a Classifier for my incoming data so that whenever I get a new movie I can guess what it's rating would be. \n",
    "   Obviously there are a few erroneous assumptions that this system jumps to out of the gate. First and most concerning of which is that it assumes that all movies of a specific genre are given similar ratings. This is obviously a pretty bad assumption to make but it turns out that it didn't matter a whole lot in the end.\n",
    "   The Process - In order to create my recommender system I first had to combine all of the data for genres into the original dataset. This was the most computationally intensive part of my recommender because for each item in my training data I had to create a new column for each genre and mark it according to which movie it was. There were probably a number of ways to optomize but I just brute forced it and ended up taking 3 hours but after it was all said and done I just saved the newly manipulated data into .dat files so that I didn't have to ever do it again.\n",
    "   After I had the seperate genres recorded I was able to utilize the scipy clustering library to cluster my data according to the matrix or genres and their corresponding rating. I started with 10 clusters and it ran relatively quickly. That is one area that I noted I would have to adjust later to see what kind of changes I could get to these results.\n",
    "   After I created the clusters I gathered them together and found their mean rating and with their standard deviation. Once I was able to get that data I still needed to create the actual recommender system itself. What I ended up doing was I created distribution over genres for every cluster that I had created. I then used those distributions to create a conditional classifier that would use a normalized version of those distributions to pick a random cluster to be a part of weighted by the distribution of the genres that that movie had in it. I then randomly selected one of the clusters from that list and used it's mean and standard deviation to sample from a gaussian to get it's rating. So for example movie 2 is an Children Adventure Fantasy movie. So for each of those genres I would look at the distribution over Children movies and randomly select a cluster according to the weights for Children movies. Then I would do the same for each of the other two genres. Say the result was an array of [0,1,0] for the cluster that those movies belonged to. Then I randomly select one of those to be the official cluster that it belongs to and use that clusters mean and standard deviation to select it's rating.\n",
    "\n",
    "3. Analysis of performance of method\n",
    "   At first I was and honestly still am worried about the amount of randomness that is inherent in this solution. The first time I ran this my algorithm all the way through I only had a 14% success rate for guessing the correct rating for the movie. It was a little discouraging but not all that surprising given that there are a lot of random samples in this method. I was however very surprised by my next discovery. Because of the randomness I wanted to see if I was even in the ballpark of getting the correct ratings. So I increased my check on percentage to a .5 error. When I increased my check to have that small margin or error my successful guesses increased to 42%! It was pretty awesome to see that despite all the randomness this algorithm was able to get almost half of it's guesses correct within a half a point. when I increase the error to 1 I get a 63% success rate, which isn't as drastic of a change as before but still pretty impressive for a simple recommender system. I actually later saw that I needed to calculate the RMSE and I have an RMSE of 1.44 which isn't too bad, but it's not great considering completely random gets around 1.99. However, because of the data that I got from the percentage I think that a majority of my guesses are either very close to spot on or they are very far off, which I think leads to the highish RMSE. This makes sense because many of my clusters had means that were centered around different numbers so if a movie is accidentally assigned to a wrong cluster then the number will be very far off."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "widgets": {
   "state": {
    "1a47f00c4bc74c44aeec037b4ff25faf": {
     "views": [
      {
       "cell_index": 3
      }
     ]
    },
    "291dfc3b2b244850b63f610473db43d9": {
     "views": [
      {
       "cell_index": 2
      }
     ]
    },
    "305471f142b64b079523bf18b1c8d963": {
     "views": [
      {
       "cell_index": 2
      }
     ]
    },
    "4c3eeed3b3c6436093919434301ea358": {
     "views": [
      {
       "cell_index": 2
      }
     ]
    },
    "4c563e2f90fc4eea83d76d961e409106": {
     "views": [
      {
       "cell_index": 3
      }
     ]
    },
    "4f569f0b7f4e4fc7b54636778d541fed": {
     "views": [
      {
       "cell_index": 2
      }
     ]
    },
    "5741e7ba679b46c7be566e70ab289486": {
     "views": [
      {
       "cell_index": 2
      }
     ]
    },
    "69da2dcc49524cf392b466aff31aef93": {
     "views": [
      {
       "cell_index": 2
      }
     ]
    },
    "9cb89aa81cb64cc78c9e45f70db56c2e": {
     "views": [
      {
       "cell_index": 5
      }
     ]
    },
    "aef02ab29db548838ab3bf461cfdaf88": {
     "views": [
      {
       "cell_index": 2
      }
     ]
    },
    "afffc38097824de9b0bd92bf9fe116e4": {
     "views": [
      {
       "cell_index": 2
      }
     ]
    },
    "b5f4d9bad31343dc960d29e9fa6d9791": {
     "views": [
      {
       "cell_index": 2
      }
     ]
    },
    "befa5f96ff8e4b468a2892af809d925a": {
     "views": [
      {
       "cell_index": 2
      }
     ]
    },
    "cf70983a4e074759a81c7876419a8d86": {
     "views": [
      {
       "cell_index": 2
      }
     ]
    },
    "d32784934cc748e4a5d54d5d03e97211": {
     "views": [
      {
       "cell_index": 2
      }
     ]
    },
    "d828f48d33914777882e1e943360f705": {
     "views": [
      {
       "cell_index": 2
      }
     ]
    },
    "e06e420392354edc9558ca7928ac5bf4": {
     "views": [
      {
       "cell_index": 2
      }
     ]
    },
    "f5f980e0c4b641379f3295bce54f6f7b": {
     "views": [
      {
       "cell_index": 3
      }
     ]
    },
    "f6b6520937dc4217a78f125de881c310": {
     "views": [
      {
       "cell_index": 0
      }
     ]
    },
    "fc710b7d3f5e44698ed37032ebde174f": {
     "views": [
      {
       "cell_index": 4
      }
     ]
    },
    "fd2a64db994a49ecaa221c9e2e205910": {
     "views": [
      {
       "cell_index": 3
      }
     ]
    }
   },
   "version": "1.2.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
